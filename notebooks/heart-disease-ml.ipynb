{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Using Machine Learning\n",
    "## Cleveland Dataset Analysis and Model Comparison\n",
    "\n",
    "This notebook implements a machine learning project to predict heart disease using the Cleveland Heart Disease dataset. We'll compare multiple machine learning algorithms and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import all the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "First, let's install all the required packages. Run this cell if you haven't installed these packages yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking installed packages:\n",
      "✓ numpy: v1.24.3 (required: >=1.21.0)\n",
      "✓ pandas: v2.0.3 (required: >=1.3.0)\n",
      "✓ scikit-learn: v1.3.2 (required: >=1.0.0)\n",
      "✓ matplotlib: v3.7.5 (required: >=3.4.0)\n",
      "✓ seaborn: v0.13.2 (required: >=0.11.0)\n",
      "✓ xgboost: v2.1.4 (required: >=1.4.0)\n",
      "✓ pytest: v8.3.5 (required: >=6.2.0)\n",
      "✓ python-dotenv: v1.0.1 (required: >=0.19.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install numpy>=1.21.0 pandas>=1.3.0 scikit-learn>=1.0.0 matplotlib>=3.4.0 seaborn>=0.11.0 xgboost>=1.4.0 pytest>=6.2.0 python-dotenv>=0.19.0\n",
    "\n",
    "# Verify installations using importlib\n",
    "import importlib\n",
    "import importlib.metadata\n",
    "\n",
    "required_packages = {\n",
    "    'numpy': '1.21.0',\n",
    "    'pandas': '1.3.0',\n",
    "    'scikit-learn': '1.0.0',\n",
    "    'matplotlib': '3.4.0',\n",
    "    'seaborn': '0.11.0',\n",
    "    'xgboost': '1.4.0',\n",
    "    'pytest': '6.2.0',\n",
    "    'python-dotenv': '0.19.0'\n",
    "}\n",
    "\n",
    "print(\"Checking installed packages:\")\n",
    "for package, min_version in required_packages.items():\n",
    "    try:\n",
    "        version = importlib.metadata.version(package)\n",
    "        print(f\"✓ {package}: v{version} (required: >={min_version})\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(f\"✗ {package}: Not installed (required: >={min_version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "Now let's import all the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steva\\AppData\\Local\\Temp\\ipykernel_43316\\723693877.py:24: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn')\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "# Set style for plotting\n",
    "plt.style.use('seaborn')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Now we'll load the Cleveland Heart Disease dataset and prepare it for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (For Colab Users)\n",
    "If you're running this notebook in Google Colab and your dataset is stored in Google Drive, run this section first to mount your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab. Using local dataset path.\n"
     ]
    }
   ],
   "source": [
    "# Check if running in Colab\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "if is_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Set the path to your dataset in Google Drive\n",
    "    # Modify this path according to your Drive structure\n",
    "    DRIVE_PATH = \"/content/drive/MyDrive/heart-disease-ml/datasets\"\n",
    "    print(f\"\\nUsing dataset path: {DRIVE_PATH}\")\n",
    "else:\n",
    "    print(\"Not running in Colab. Using local dataset path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully from: ../datasets/processed.cleveland.data\n",
      "\n",
      "First few rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
       "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
       "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
       "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
       "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
       "\n",
       "   slope   ca  thal  target  \n",
       "0    3.0  0.0   6.0       0  \n",
       "1    2.0  3.0   3.0       2  \n",
       "2    2.0  2.0   7.0       1  \n",
       "3    3.0  0.0   3.0       0  \n",
       "4    1.0  0.0   3.0       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        299 non-null    float64\n",
      " 12  thal      301 non-null    float64\n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(13), int64(1)\n",
      "memory usage: 33.3 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define column names for the dataset\n",
    "COLUMN_NAMES = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "# Set the dataset path based on environment\n",
    "if is_colab():\n",
    "    dataset_path = f\"{DRIVE_PATH}/processed.cleveland.data\"\n",
    "else:\n",
    "    dataset_path = \"../datasets/processed.cleveland.data\"\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path, names=COLUMN_NAMES, na_values=\"?\")\n",
    "    print(f\"Dataset loaded successfully from: {dataset_path}\")\n",
    "    \n",
    "    # Display the first few rows and basic information\n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    display(df.head())\n",
    "    print(\"\\nDataset information:\")\n",
    "    display(df.info())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the dataset at {dataset_path}\")\n",
    "    if is_colab():\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You have mounted Google Drive\")\n",
    "        print(\"2. The dataset is in the correct location in your Drive\")\n",
    "        print(f\"3. The path '{DRIVE_PATH}' is correct for your Drive structure\")\n",
    "    else:\n",
    "        print(\"\\nPlease ensure the dataset is in the 'datasets' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (242, 13)\n",
      "Testing set shape: (61, 13)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Handle missing values\n",
    "    df = df.replace(\"?\", np.nan)\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    df = df.fillna(df.median())\n",
    "    \n",
    "    # Convert target to binary (0: no disease, 1: disease)\n",
    "    df['target'] = df['target'].map(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to DataFrame to preserve column names\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network model\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "def create_neural_network():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(32, activation='relu', input_shape=(13,)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Wrapper class for Keras model to match scikit-learn API\n",
    "class KerasWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model=None):\n",
    "        self.model = model if model is not None else create_neural_network()\n",
    "        self.classes_ = np.array([0, 1])\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y, epochs=50, batch_size=32, verbose=0)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return (self.model.predict(X) > 0.5).astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        probs = self.model.predict(X)\n",
    "        return np.column_stack([1 - probs, probs])\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"model\": self.model}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "We'll train and evaluate multiple models:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Support Vector Machine (SVM)\n",
    "4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training base models...\n",
      "Training logistic_regression...\n",
      "Training random_forest...\n",
      "Training svm...\n",
      "Training xgboost...\n",
      "Training neural_network...\n",
      "Training neural_network...\n",
      "\n",
      "Creating and training ensemble models...\n",
      "\n",
      "Training ensemble models...\n",
      "\n",
      "Creating and training ensemble models...\n",
      "\n",
      "Training ensemble models...\n"
     ]
    }
   ],
   "source": [
    "# Create and train base models first\n",
    "base_models = {\n",
    "    \"logistic_regression\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    \"random_forest\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    \"svm\": SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    \"xgboost\": XGBClassifier(random_state=RANDOM_STATE),\n",
    "    \"neural_network\": KerasWrapper()\n",
    "}\n",
    "\n",
    "# Train base models first\n",
    "print(\"Training base models...\")\n",
    "trained_base_models = {}\n",
    "for name, model in base_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    trained_base_models[name] = model.fit(X_train, y_train)\n",
    "\n",
    "# Create ensemble models with trained base models\n",
    "print(\"\\nCreating and training ensemble models...\")\n",
    "estimators = [(name, model) for name, model in trained_base_models.items()]\n",
    "\n",
    "# Create voting classifier without neural network first\n",
    "voting_estimators = [(name, model) for name, model in trained_base_models.items() \n",
    "                    if name != \"neural_network\"]\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "# Create stacking classifiers\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=voting_estimators,  # Use same estimators as voting\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# For full stacking, include neural network\n",
    "full_stacking_estimators = [(name, model) for name, model in trained_base_models.items()]\n",
    "full_stacking_clf = StackingClassifier(\n",
    "    estimators=full_stacking_estimators,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train ensemble models\n",
    "print(\"\\nTraining ensemble models...\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "full_stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Combine all models in final dictionary\n",
    "trained_models = {\n",
    "    **trained_base_models,\n",
    "    \"voting\": voting_clf,\n",
    "    \"stacking\": stacking_clf,\n",
    "    \"full_stacking\": full_stacking_clf\n",
    "}\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    results[name] = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Display results as a DataFrame\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Let's create visualizations to better understand our models' performance:\n",
    "1. ROC Curves\n",
    "2. Confusion Matrices\n",
    "3. Feature Importance (where available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for supported models\n",
    "def get_feature_importance(model, feature_names):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return pd.Series(model.feature_importances_, index=feature_names)\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return pd.Series(np.abs(model.coef_[0]), index=feature_names)\n",
    "    return None\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "supported_models = ['logistic_regression', 'random_forest', 'xgboost']\n",
    "\n",
    "for idx, name in enumerate(supported_models):\n",
    "    importance = get_feature_importance(trained_models[name], X_train.columns)\n",
    "    if importance is not None:\n",
    "        importance.sort_values().plot(kind='barh', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Feature Importance\\n{name}')\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "From our analysis:\n",
    "\n",
    "1. Model Performance:\n",
    "   - All models achieved good performance with accuracy > 85%\n",
    "   - Random Forest achieved the best performance (90.16% accuracy, 0.959 AUC)\n",
    "   - Neural Network and XGBoost showed strong performance\n",
    "   - Ensemble methods (Stacking, Voting) provided robust results\n",
    "\n",
    "2. Feature Importance:\n",
    "   - The most important features vary between models\n",
    "   - Common important features include: cp (chest pain type), thalach (maximum heart rate), and oldpeak (ST depression)\n",
    "\n",
    "3. Ensemble Methods:\n",
    "   - Voting classifier provided stable predictions\n",
    "   - Full stacking with all models including neural network showed good performance\n",
    "   - Different ensemble approaches offer trade-offs between performance and complexity\n",
    "\n",
    "4. Recommendations:\n",
    "   - Random Forest could be the preferred model due to its balance of performance and interpretability\n",
    "   - Consider using ensemble methods for critical applications\n",
    "   - Further hyperparameter tuning could potentially improve performance\n",
    "   - Consider collecting more data to improve model robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JM7TQfVZRaBc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONfBL++k2HmPdzg/QsOY2e",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "HEARTDISEASEML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
