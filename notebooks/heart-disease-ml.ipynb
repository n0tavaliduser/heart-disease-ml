{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Using Machine Learning\n",
    "## Cleveland Dataset Analysis and Model Comparison\n",
    "\n",
    "This notebook implements a machine learning project to predict heart disease using the Cleveland Heart Disease dataset. We'll compare multiple machine learning algorithms and evaluate their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import all the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package Installation\n",
    "First, let's install all the required packages. Run this cell if you haven't installed these packages yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install numpy>=1.21.0 pandas>=1.3.0 scikit-learn>=1.0.0 matplotlib>=3.4.0 seaborn>=0.11.0 xgboost>=1.4.0 pytest>=6.2.0 python-dotenv>=0.19.0\n",
    "\n",
    "# Verify installations using importlib\n",
    "import importlib\n",
    "import importlib.metadata\n",
    "\n",
    "required_packages = {\n",
    "    'numpy': '1.21.0',\n",
    "    'pandas': '1.3.0',\n",
    "    'scikit-learn': '1.0.0',\n",
    "    'matplotlib': '3.4.0',\n",
    "    'seaborn': '0.11.0',\n",
    "    'xgboost': '1.4.0',\n",
    "    'pytest': '6.2.0',\n",
    "    'python-dotenv': '0.19.0'\n",
    "}\n",
    "\n",
    "print(\"Checking installed packages:\")\n",
    "for package, min_version in required_packages.items():\n",
    "    try:\n",
    "        version = importlib.metadata.version(package)\n",
    "        print(f\"✓ {package}: v{version} (required: >={min_version})\")\n",
    "    except importlib.metadata.PackageNotFoundError:\n",
    "        print(f\"✗ {package}: Not installed (required: >={min_version})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "Now let's import all the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "# Set style for plotting\n",
    "plt.style.use('seaborn')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Now we'll load the Cleveland Heart Disease dataset and prepare it for our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Setup (For Colab Users)\n",
    "If you're running this notebook in Google Colab and your dataset is stored in Google Drive, run this section first to mount your Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "def is_colab():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Mount Google Drive if in Colab\n",
    "if is_colab():\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully!\")\n",
    "    \n",
    "    # Set the path to your dataset in Google Drive\n",
    "    # Modify this path according to your Drive structure\n",
    "    DRIVE_PATH = \"/content/drive/MyDrive/heart-disease-ml/datasets\"\n",
    "    print(f\"\\nUsing dataset path: {DRIVE_PATH}\")\n",
    "else:\n",
    "    print(\"Not running in Colab. Using local dataset path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names for the dataset\n",
    "COLUMN_NAMES = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "\n",
    "# Set the dataset path based on environment\n",
    "if is_colab():\n",
    "    dataset_path = f\"{DRIVE_PATH}/processed.cleveland.data\"\n",
    "else:\n",
    "    dataset_path = \"datasets/processed.cleveland.data\"\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(dataset_path, names=COLUMN_NAMES, na_values=\"?\")\n",
    "    print(f\"Dataset loaded successfully from: {dataset_path}\")\n",
    "    \n",
    "    # Display the first few rows and basic information\n",
    "    print(\"\\nFirst few rows of the dataset:\")\n",
    "    display(df.head())\n",
    "    print(\"\\nDataset information:\")\n",
    "    display(df.info())\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Could not find the dataset at {dataset_path}\")\n",
    "    if is_colab():\n",
    "        print(\"\\nPlease ensure:\")\n",
    "        print(\"1. You have mounted Google Drive\")\n",
    "        print(\"2. The dataset is in the correct location in your Drive\")\n",
    "        print(f\"3. The path '{DRIVE_PATH}' is correct for your Drive structure\")\n",
    "    else:\n",
    "        print(\"\\nPlease ensure the dataset is in the 'datasets' folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "def preprocess_data(df):\n",
    "    # Handle missing values\n",
    "    df = df.replace(\"?\", np.nan)\n",
    "    df = df.apply(pd.to_numeric)\n",
    "    df = df.fillna(df.median())\n",
    "    \n",
    "    # Convert target to binary (0: no disease, 1: disease)\n",
    "    df['target'] = df['target'].map(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Convert to DataFrame to preserve column names\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "# Preprocess the data\n",
    "X_train, X_test, y_train, y_test = preprocess_data(df)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "We'll train and evaluate multiple models:\n",
    "1. Logistic Regression\n",
    "2. Random Forest\n",
    "3. Support Vector Machine (SVM)\n",
    "4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train models\n",
    "models = {\n",
    "    \"logistic_regression\": LogisticRegression(random_state=RANDOM_STATE, max_iter=1000),\n",
    "    \"random_forest\": RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    \"svm\": SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    \"xgboost\": XGBClassifier(random_state=RANDOM_STATE)\n",
    "}\n",
    "\n",
    "# Train all models\n",
    "trained_models = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    trained_models[name] = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'auc': roc_auc_score(y_test, y_pred_proba)\n",
    "    }\n",
    "\n",
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in trained_models.items():\n",
    "    print(f\"Evaluating {name}...\")\n",
    "    results[name] = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "# Display results as a DataFrame\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "print(\"\\nModel Performance Metrics:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Let's create visualizations to better understand our models' performance:\n",
    "1. ROC Curves\n",
    "2. Confusion Matrices\n",
    "3. Feature Importance (where available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (name, model) in enumerate(trained_models.items()):\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n",
    "    axes[idx].set_title(f'{name}\\nConfusion Matrix')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for supported models\n",
    "def get_feature_importance(model, feature_names):\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        return pd.Series(model.feature_importances_, index=feature_names)\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        return pd.Series(np.abs(model.coef_[0]), index=feature_names)\n",
    "    return None\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "supported_models = ['logistic_regression', 'random_forest', 'xgboost']\n",
    "\n",
    "for idx, name in enumerate(supported_models):\n",
    "    importance = get_feature_importance(trained_models[name], X_train.columns)\n",
    "    if importance is not None:\n",
    "        importance.sort_values().plot(kind='barh', ax=axes[idx])\n",
    "        axes[idx].set_title(f'Feature Importance\\n{name}')\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "        \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "From our analysis:\n",
    "\n",
    "1. Model Performance:\n",
    "   - All models achieved good performance with accuracy > 85%\n",
    "   - Random Forest and XGBoost performed slightly better than other models\n",
    "   - SVM showed good performance but with slightly lower metrics\n",
    "\n",
    "2. Feature Importance:\n",
    "   - The most important features vary between models\n",
    "   - Common important features include: cp (chest pain type), thalach (maximum heart rate), and oldpeak (ST depression)\n",
    "\n",
    "3. Recommendations:\n",
    "   - Random Forest could be the preferred model due to its balance of performance and interpretability\n",
    "   - Further hyperparameter tuning could potentially improve performance\n",
    "   - Consider collecting more data to improve model robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JM7TQfVZRaBc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONfBL++k2HmPdzg/QsOY2e",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
